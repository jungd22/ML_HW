{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Group 1\n",
    "#### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David Jung, Ryan Voges, Emily Blake, Spencer Powell, Abraham Alhomadi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise I want you to apply KNN regression model to the bikeshare data set which is available on the GitHub folder for HW7. The data are 17379 observations of hourly counts from 2011 to 2012 for bike rides (rentals) from the Capital Bikeshare system in Washington DC. It was originally compiled by Fanaee and Gama in ‘Event labeling combining ensemble detectors and background knowledge’ (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be qualified to get full mark if you beat the following performance metrics: \n",
    "\n",
    "Question 1:        RMSE_test = 120\n",
    "\n",
    "Question 2:   error_rate_test = 0.05 when threshold =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: KNN Regression (45 points)\n",
    "\n",
    "Import the bikeshare.csv as a data frame and name it as df. bikeshare.csv contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bikeshare.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = [\"mnth\", \"hr\", \"weathersit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first = True, columns = dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop the dteday variable and then define your feature space and target variables. Split the data into test (20%) and train set (80%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"dteday\", axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our target variable and the features and splitting the dataset into train and test:\n",
    "y = df['cnt']\n",
    "X = df.drop('cnt', axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From sklearn.neighbors import the relevant function for KNN regression. Do the followings: (25 points)\n",
    "    1. Train all the model with the default features. (5 points)\n",
    "    2.Make predictions on the test set and save them as y_hat (5 points)\n",
    "    3. Construct a data frame named df_predictions with 2 columns. y_test, and y_hat  from previous part (5 points)\n",
    "    4. Visualize actual vs predicted counts in the test set using an scatterplot. Are you visually satisfied with the regression model?\n",
    "    5. Report the RMSE_test for the KNN regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_regression = KNeighborsRegressor(n_neighbors=5)\n",
    "KNN_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_hat = KNN_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame({'y_test':y_test, 'y_hat':y_hat})\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_hat, alpha=0.6)\n",
    "sns.lineplot(y_test, y_test)\n",
    "\n",
    "plt.xlabel('Actual Total Bikes Rented', fontsize=14)\n",
    "plt.ylabel('Prediced  Total Bikes Rented', fontsize=14)\n",
    "plt.title('Actual vs Predicted  Count (test set)', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_test = round(np.mean(np.square(y_test - y_hat)),2)\n",
    "MSE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_test = round(np.sqrt(MSE_test),2)\n",
    "RMSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cross validation: (15 points)\n",
    "    1. Estimate the RMSE_test by doing a 5 fold cross validation on the train set and name it as RMSE_CV. (5 points)\n",
    "    2. Plot the RMSE_CV vs K and find the optimal value for K in the KNN regression model. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMSE = cross_val_score(estimator = KNN_regression, X = X_train, y = y_train, cv = 5, scoring=\"neg_mean_squared_error\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_CV = round(np.mean(-NMSE),4)\n",
    "MSE_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_CV = round(np.sqrt(MSE_CV), 4)\n",
    "RMSE_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_CV=[]\n",
    "RMSE_test = []\n",
    "\n",
    "k=20\n",
    "\n",
    "for i in range(1,k):\n",
    "    KNN_i = KNeighborsRegressor(n_neighbors=i)\n",
    "    KNN_i.fit(X_train, y_train)\n",
    "    RMSE_i = np.sqrt(np.mean(-1*cross_val_score(estimator = KNN_i, X = X_train, y = y_train, cv = 5, scoring=\"neg_mean_squared_error\" )))\n",
    "    RMSE_CV.append(RMSE_i)\n",
    "    \n",
    "    RMSE_test.append(np.sqrt(np.mean(np.square(y_test - KNN_i.predict(X_test)))))\n",
    "    \n",
    "optimal_k = pd.DataFrame({'RMSE_CV': np.round(RMSE_CV,2), 'RMSE_test':np.round(RMSE_test,2)}, index=range(1,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(data=optimal_k)\n",
    "plt.title('Cross Validated RMSE VS K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We would choose a K of 3 because this is when the RMSE_CV is the lowest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: KNN Classification (105 points)\n",
    "\n",
    "The managers of Capital Bikeshare have found that the system works smoothly until more than 500 bikes are rented in any one hour. At that point, it becomes necessary to insert extra bikes into the system and move them across stations to balance loads. Do the followings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define a binary target variable overload. Overload=1 if cnt>500 and 0 otherwise. What are the proportions of overload vs non-overload in your data set? Is the target variable balanced or imbalanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"overload\"] = np.where(df[\"cnt\"] > 500, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we look at target variable proportions:\n",
    "pd.crosstab(df['overload'],df['overload'], normalize='all')*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our data is very imbalanced, but this is to be expected. It is not very often that we woud expect 500 bikes to be rented in any one hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Along with the target variable, define your feature space (X) and split the data into test (30%) and train set (70%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['overload']\n",
    "X = df.drop('overload', axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. From sklearn.neighbors import the relevant function for KNN classification. Do the followings: (25 points)\n",
    "    1. Train the KNN classification model using its default parameters. (5 points)\n",
    "    2. Generate the predicted probabilities and predicted classifications and save them as y_hat_probs, y_hat respectively. (5 points)\n",
    "    3. Plot the histogram of y_hat_probs? Explain what you see? Is there a probability threshold at which the model always predict negative or positive? (5 points)\n",
    "    4. Generate predicted classifications for two different thresholds (30% and 70% threshold). Save these new predictions as y_hat_30 and y_hat_70. Which threshold should you use if your goal is to avoid too many false negatives? Explain your answer. (10 points)\n",
    "    5. Construct a data frame named df_predictions with 5 columns. y_test,  and the 4 y_hats from previous parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting KNN classifier to the Training set\n",
    "\n",
    "KNN_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minkowski distance: https://en.wikipedia.org/wiki/Minkowski_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set probabilities and classes\n",
    "y_hat      = KNN_classifier.predict(X_test)\n",
    "y_hat_probs = KNN_classifier.predict_proba(X_test)[:,1] \n",
    "# predicted probabilities are reported for both classes. we saved the prob of overloads or no overloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(KNN_classifier.predict_proba(X_test),3)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the histogram of probabilities of default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_hat_probs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We would say the histogram shows a fair distribution, because we knew that our data is relatively imbalanced. Therefore, we expect that the probability that an overload would exist within one hour is highly unlikely, which is what is communicated by the histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_30 = np.where(y_hat_probs>0.3,1,0)\n",
    "y_hat_70 = np.where(y_hat_probs>0.7,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put all the predictions together in a data frame.\n",
    "df_predictions = pd.DataFrame({'y_test':y_test, 'y_hat_probs':y_hat_probs,\n",
    "                               'y_hat_30':y_hat_30, 'y_hat_70':y_hat_70})\n",
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If our objective is to reduce false negatives, we would choose the lower threshold. Therfore, we would go with 30%. Simply put, we are lowering the bar, so we could avoid our model from incorrectly classifying the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_0 = np.where(y_hat_probs>0,1,0)\n",
    "y_hat_1 = np.where(y_hat_probs>1,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions2 = pd.DataFrame({'y_test':y_test, 'y_hat_probs':y_hat_probs,\n",
    "                               'y_hat_0':y_hat_0, 'y_hat_1': y_hat_1})\n",
    "df_predictions2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_hat_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_hat_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We would only be able to create a threshold of 1 that would predict all data points as negative. Setting a threshold equal to 0 would still give us similar predictions of setting a threshold between 0 and 1. This is because there are very few probabilities in between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Borrow my_KNN_report() function from the python notebook of class 13. (25 points) \n",
    "    1. Report the Accuracy, precision, recall and f1 score along with the confusion matrix for threshold =0.5. Interpret all these statistics. Do you trust the accuracy of the model? why? (15 points)\n",
    "    2. Now use threshold = 0.3 in the my_logistic_report() function. what happens to accuracy, precision, recall and f1 score? what happens to false negatives? is this consistent with you answer to question 5.4? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "def my_KNN_report(X_train, y_train, X_test,y_test, K=5, threshold=0.5):\n",
    "    knn= KNeighborsClassifier(n_neighbors=K)\n",
    "    knn.fit(X_train, y_train)\n",
    "    probs = knn.predict_proba(X_test)[:,1]\n",
    "    y_hat = np.where(probs>=threshold,1,0)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_hat)\n",
    "    accuracy = round(accuracy_score(y_test,y_hat) ,4)\n",
    "    error_rate = round(1-accuracy,4)\n",
    "    precision = round(precision_score(y_test,y_hat),2)\n",
    "    recall = round(recall_score(y_test,y_hat),2)\n",
    "    f1score = round(f1_score(y_test,y_hat),2)\n",
    "    cm_labled = pd.DataFrame(cm, index=['Actual : negative ','Actual : positive'], columns=['Predict : negative','Predict :positive '])\n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "    print('Accuracy  = {}'.format(accuracy))\n",
    "    print('Error_rate  = {}'.format(error_rate))\n",
    "    print('Precision = {}'.format(precision))\n",
    "    print('Recall    = {}'.format(recall))\n",
    "    print('f1_score  = {}'.format(f1score))\n",
    "    print(\"-----------------------------------------\")\n",
    "    return cm_labled\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_KNN_report(X_train, y_train, X_test,y_test, K=5, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since this is a large data set and we know the distribution of the data, we can feel confident in our accuracy score. We also calculated an f1 score of 1.0, which means that our model does a great job of predicting both negative and positive overload probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_KNN_report(X_train, y_train, X_test,y_test, K=5, threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our false negatives decreased to 0 and our false positives increased to 1. This in turn increased our accuracy to .9998 and our recall to 1.0. I would still say that our model does a great job predicting both negative and positive overload probabilities, but a threshold of 0.3 is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the ROC curve and report the AUC score. Is your model doing a better job than random prediction (no skill)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a no skill (ns) prediction and KNN  (lr) predictions.\n",
    "ns_probs = [0 for i in range(len(y_test))]\n",
    "lr_probs = KNN_classifier.predict_proba(X_test)[:,1]\n",
    "# calculating scores for no skill and KNN \n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('KNN: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='KNN')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not to flex, but our model is doing an AMAZING job of predicting overload probabilities compared to the random prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Cross validation: (15 points)\n",
    "    1. Estimate the error_rate_test by doing a 5 fold cross validation on the train set and name it as error_rate_CV. (5 points)\n",
    "    2. Plot the error_rate_CV vs K and find the optimal value for K in the KNN classification model. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(estimator = KNN_classifier, X = X_train, y = y_train, cv = 5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_CV=round((1-accuracy).mean(),4)\n",
    "error_rate_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "k=50\n",
    "\n",
    "for i in range(1,k):\n",
    "    KNN_i = KNeighborsClassifier(n_neighbors=i)\n",
    "    KNN_i.fit(X_train, y_train)\n",
    "    error_rate.append(np.mean(y_test != KNN_i.predict(X_test)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(y=error_rate, x=range(1,k))\n",
    "plt.title('Test Error Rate VS K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our optimal K is 35. This is when the error rate is minimized and our KNN model is the simplest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. As the manager of Capital Bikeshare, you are dealing with a trade of between unexpected overload cost and cost of idle bikes. If the cost of a single idle bike is smaller than the cost of a single unexpected overload, then which of the following probability thresholds would satisfy your objective? 0.3, 0.5 or 0.7? (15 points)  Hint: idle bike = False overload and unexpected overload = False non-overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idle bike = FP\n",
    "# unexpected overload = FN\n",
    "# We want to trade an idle bike for unexpected overload (minimize false negatives)\n",
    "\n",
    "my_KNN_report(X_train, y_train, X_test,y_test, K=5, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_KNN_report(X_train, y_train, X_test,y_test, K=5, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_KNN_report(X_train, y_train, X_test,y_test, K=5, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given this trade-off between idle bikes and unexpected overloads, we would choose a threshold of 0.3. This minimizes our # of unexpected overloads (false negatives) to 0 and trades any potential overloads for an idle bike (false positive). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
